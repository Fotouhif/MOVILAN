This repository is currently under construction

to get started

first go to home/homagni/Docker-recipes/vision_language/Dockerfile

the dockerfile is the complete specification of my vision laguage environment, there you can also add additional dependencies towards the end and recompile the docker image (docker knowhows in homagn/github)

there towards the end you will see how to also launch a docker container of the current image -> (Execution instructions -Ai2Thor). By following that you can launch an instance of MOVILAN

inside the docker image whatever python files you can run, run them with the python3 command 

check the Dockerfile to see where you can add your own new libraries
after adding the libraries you can recompile the dockerfile using 
sudo nvidia-docker build -t homagni/vision_language:latest .



Language understanding:

the language_understanding folder is for the BERT part it is complete as module, the main file is instruction_parser.py which will ask the user for instructions when running, but to integrate into MOVILAN, still need to look at the batch processing instruction parsing (the one with (b) option- how to include it in the main ai2thor execution instruction code)

Mapping

the mapper/ folder is currently under scrutiny. Almost organized code, but need to check the data generation process for all different rooms (other than bedrooms that are in 300-400 range). For now the main code is -> test_gcn.py and train_gcn.py /  need to integrate those in the main execution code that was in hutils/navigation_signatures.py
Todo:
write up a stereo depth estimation algorithm- this can be done using a siamese autoencoder. Inputs are taken from two RGB cameras placed small distance apart, 
and the goal for the autoencoder is to predict the depth normals (just like predicting image segmentation). This may be also done in a patch wise fashion over
entire panoramic images (each patch also rotated up,down,left,right) to reduce bias on specific scenes 
data collection-> just nudge the agent slightly left and right to collect a stereo image of the location/ target true depth normal already provided by the simulator
Todo:
Also train a seperate network for semantic image segmentation and not use the gold standard segmentation
the semantic segmentation can be trained as a collection of set of predictors. Each predictor can segment upto 10 unique classes (including nothing)
and min of 2 classes (one the specific object and then nothing or background )
This way an unlimited number of predictors can be stacked to work together and provide segmentation for any scene by majority votings

robot

this is an abstraction package to the ai2thor/alfred library. Need to write function so that MOVILAN can be generalized to any vision language robot application